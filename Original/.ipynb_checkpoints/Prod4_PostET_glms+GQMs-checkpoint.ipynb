{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c932e27",
   "metadata": {},
   "source": [
    "# GLMS -- SingleRFs "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a31433",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "expected an indented block (2676782858.py, line 17)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Input \u001b[0;32mIn [1]\u001b[0;36m\u001b[0m\n\u001b[0;31m    else:\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m expected an indented block\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import h5py \n",
    "\n",
    "# setup paths\n",
    "iteration = 1 # which version of this tutorial to run (in case want results in different dirs)\n",
    "NBname = 'color_cloud_initial{}'.format(iteration)\n",
    "\n",
    "myhost = os.uname()[1] # get name of machine\n",
    "print(\"Running on Computer: [%s]\" %myhost)\n",
    "\n",
    "if myhost=='mt': # this is sigur\n",
    "    \n",
    "#    sys.path.insert(0, '/home/jake/Repos/')\n",
    "#    dirname = os.path.join('.', 'checkpoints')\n",
    "#    datadir = '/home/dbutts/V1/Conway/'\n",
    "else:\n",
    "    sys.path.insert(0, '/home/felixbartsch/Code/') \n",
    "    datadir = '/Data/FelixData/Conway/'  \n",
    "    dirname = '/home/felixbartsch/Data/Colorworkspace/' # Working directory \n",
    "\n",
    "import numpy as np\n",
    "import scipy.io as sio\n",
    "from copy import deepcopy\n",
    "\n",
    "# plotting\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import torch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# NDN tools\n",
    "import NDNT.utils as utils # some other utilities\n",
    "from NDNT.utils import imagesc   # because I'm lazy\n",
    "from NDNT.utils import ss        # because I'm real lazy\n",
    "import NDNT.NDNT as NDN\n",
    "from NDNT.modules.layers import *\n",
    "from NDNT.networks import *\n",
    "from time import time\n",
    "import dill\n",
    "import ColorDataUtils.ConwayUtils as CU\n",
    "import ColorDataUtils.RFutils as RU\n",
    "\n",
    "from NTdatasets.generic import GenericDataset\n",
    "import NTdatasets.conway.cloud_datasets as datasets\n",
    "import NTdatasets.conway.bar1d_datasets as bardatasets\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "device0 = torch.device(\"cpu\")\n",
    "dtype = torch.float32\n",
    "\n",
    "# Where saved models and checkpoints go -- this is to be automated\n",
    "print( 'Save_dir =', dirname)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "626a6659",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Load BINOCULAR only (single RFs)\n",
    "# Load data\n",
    "fn = 'Jocamo_220801_full_CC_ETCC_nofix_v08'\n",
    "fndate = '0801'\n",
    "dirname2 = dirname+fndate+'/'\n",
    "dirname_mod = dirname+fndate+'/models/'\n",
    "\n",
    "UNX = 52\n",
    "num_lags = 10\n",
    "\n",
    "inclMUs = True\n",
    "t0 = time()\n",
    "data = datasets.ColorClouds(\n",
    "    filenames=[fn], eye_config=3, drift_interval=16,\n",
    "    datadir=datadir, folded_lags=False, luminance_only=False,\n",
    "    trial_sample=True, num_lags=num_lags, \n",
    "    include_MUs=inclMUs)\n",
    "t1 = time()\n",
    "print(t1-t0, 'sec elapsed')\n",
    "\n",
    "lam_units = np.where(data.channel_map < 32)[0]\n",
    "ETunits = np.where(data.channel_map > 32)[0]\n",
    "UTunits = np.where(data.channel_map >= 32+127)[0]\n",
    "NFunits = np.where((data.channel_map > 32) & (data.channel_map < (32+128)))[0]\n",
    "print( \"%d laminar units, %d ET units\"%(len(lam_units), len(ETunits)))\n",
    "\n",
    "# Pull correct saccades\n",
    "matdat = sio.loadmat( datadir+fn+'_ETupdate.mat')\n",
    "data.process_fixations(matdat['sac_binsB'][0, :])\n",
    "\n",
    "sac_ts_all = matdat['ALLsac_bins'][0, :]\n",
    "#data.process_fixations( sac_ts_all )\n",
    "\n",
    "#    'tsacs_msB': Bsac_ts,  ## microsaccade times in seconds (4 sec trials)\n",
    "#    'sac_bins_allB': Bsacbins_all, # bin numbers of sac sampled at 60 Hz\n",
    "#    'sac_amps_allB': CampsB, ## squared magnitude\n",
    "#    'sac_ampsB': Bsac_amps, # reduced sac times & amplitude given threshold\n",
    "#    'sac_binsB': Bsac_bins,\n",
    "#    'sac_tsB': Bsac_ts,\n",
    "#    'et60HzB': ETprocB, # downsampled processed eye trace\n",
    "#    'et1kHzB': et1khzB,\n",
    "\n",
    "NT = len(data.fix_n)\n",
    "NA = data.Xdrift.shape[1]\n",
    "print(\"%d (%d valid) time points\"%(NT, len(data)))\n",
    "\n",
    "# # Replace DFs\n",
    "matdat = sio.loadmat(datadir+fn+'_DFextra.mat')\n",
    "data.dfs = torch.tensor( matdat['XDF'][:NT, :], dtype=torch.float32 )\n",
    "data.dfs.shape\n",
    "\n",
    "#shift robs to test weird NLRF alignment\n",
    "#data.robs=torch.from_numpy(np.roll(data.robs,shift=8,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e49f9f30",
   "metadata": {},
   "outputs": [],
   "source": [
    "SHfile = sio.loadmat(dirname2 + 'J22'+fndate+'_bestshifts.mat')\n",
    "#SHfile = sio.loadmat(dirname2 + 'BDshifts0628i3.mat')\n",
    "fix_n  = SHfile['fix_n']\n",
    "fixshifts = SHfile['shifts']\n",
    "metricsLL = SHfile['metricsLL']\n",
    "metricsTH = SHfile['metricsTH']\n",
    "ETshifts  = SHfile['ETshifts']\n",
    "ETmetrics = SHfile['ETmetrics']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99968b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## only use most confident fixations\n",
    "goodfix = np.where(ETmetrics[:,1] < 0.99)\n",
    "valfix = torch.zeros([ETmetrics.shape[0], 1], dtype=torch.float32)\n",
    "valfix[goodfix] = 1.0\n",
    "# Test base-level performance (full DFs and then modify DFs)\n",
    "data.dfs=np.multiply(data.dfs,valfix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60fb3b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Segregate 'valid' Utah units\n",
    "Reff = torch.mul(data.robs[:, UTunits], data.dfs[:, UTunits]).numpy()\n",
    "nspks = np.sum(Reff, axis=0)\n",
    "a = np.where(nspks > 10)[0]\n",
    "valUT = UTunits[a]\n",
    "NCUT = len(valUT)\n",
    "\n",
    "Reff = torch.mul(data.robs[:, NFunits], data.dfs[:, NFunits]).numpy()\n",
    "nspks = np.sum(Reff, axis=0)\n",
    "a = np.where(nspks > 10)[0]\n",
    "valNF = NFunits[a]\n",
    "NCNF = len(valNF)\n",
    "\n",
    "Reff = torch.mul(data.robs[:, ETunits], data.dfs[:, ETunits]).numpy()\n",
    "nspks = np.sum(Reff, axis=0)\n",
    "a = np.where(nspks > 100)[0]\n",
    "valET = ETunits[a]\n",
    "NCv_ET = len(valET)\n",
    "\n",
    "Reff = torch.mul(data.robs[:, lam_units], data.dfs[:, lam_units]).numpy()\n",
    "nspks = np.sum(Reff, axis=0)\n",
    "a = np.where(nspks > 10)[0]\n",
    "valLP = lam_units[a]\n",
    "NCL = len(valLP)\n",
    "NCv = len(valLP)\n",
    "\n",
    "print( \"%d out of %d Laminar units kept\"%(NCL, len(lam_units)) )\n",
    "print( \"%d out of %d NF units kept\"%(NCNF, len(NFunits)) )\n",
    "print( \"%d out of %d Utah units kept\"%(NCUT, len(UTunits)) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "419cd63d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# New GQM fits:\n",
    "top_corner = np.array([970, 540], dtype=np.int64) #for 0808 - LP meas done\n",
    "#top_corner = np.array([980, 530], dtype=np.int64) #for 0722 - LP meas done; ET done; GQMs done\n",
    "\n",
    "# OLD fits/ TODO:\n",
    "#TODO with new 1Dbar-compatible loading:\n",
    "#for 0207\n",
    "#for 0217\n",
    "#for 0304\n",
    "#top_corner = np.array([966, 535], dtype=np.int64) #for 0314\n",
    "#for 0316\n",
    "#for 0318\n",
    "#top_corner = np.array([966, 530], dtype=np.int64) #for 0320 - LP meas done\n",
    "#for 0321\n",
    "\n",
    "# 0616\n",
    "#top_corner = np.array([955, 525], dtype=np.int64) #for 0621 -LP meas done (1D ET)\n",
    "\n",
    "#have ET clouds:\n",
    "#top_corner = np.array([950, 530], dtype=np.int64) #for 0628 - LP meas done; ET done (previously [965, 535] - may ned to fit second set of STAs with new location)\n",
    "#top_corner = np.array([950, 530], dtype=np.int64) # 0701 - LP meas done\n",
    "#top_corner = np.array([965, 530], dtype=np.int64) # 0705 - LP meas done\n",
    "#top_corner = np.array([960, 535], dtype=np.int64) #for 0707 - LP meas done\n",
    "# 0711\n",
    "#top_corner = np.array([955, 535], dtype=np.int64) #for 0713 - LP meas done\n",
    "#top_corner = np.array([938, 512], dtype=np.int64) #for 0715 - All meas done\n",
    "#top_corner = np.array([950, 530], dtype=np.int64) #for 0718 - LP meas done; ET done\n",
    "#top_corner = np.array([995, 545], dtype=np.int64) #for 0720 - LP meas done\n",
    "\n",
    "#for 0725 - Ethan sorted\n",
    "#top_corner = np.array([1000, 555], dtype=np.int64) #for 0727 - LP meas done\n",
    "#top_corner = np.array([980, 540], dtype=np.int64) #for  0801 - LP meas done; ET needs to be redone; previously [990, 550]\n",
    "#top_corner = np.array([985, 560], dtype=np.int64) #for 0803 -LP meas done\n",
    "#\n",
    "# 0909\n",
    "# top_corner = np.array([965, 530], dtype=np.int64) #for 0921 - LP meas done\n",
    "# top_corner = np.array([1020, 545], dtype=np.int64) #for 0926 - LP meas done\n",
    "#top_corner = np.array([1005, 530], dtype=np.int64) #for 1003 - LP meas done\n",
    "#top_corner = np.array([1015, 540], dtype=np.int64) #for 1007 - LP meas done\n",
    "\n",
    "data.draw_stim_locations(top_corner = top_corner, L=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6153a0d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#data.assemble_stimulus(which_stim='lam', stim_wrap = [30, -25], fixdot=0 ) #, stim_crop=[0,49,5,54])\n",
    "\n",
    "data.assemble_stimulus(top_corner=top_corner, fixdot=0, L=60, shifts=-fixshifts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88abc2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LP STAs\n",
    "Reff = torch.mul(data.robs[:, valLP], data.dfs[:, valLP])\n",
    "nspks = torch.sum(Reff, axis=0)\n",
    "lag = 3\n",
    "#stas0 = ((data.stim[:-lag, ...].T @ Reff[lag:,:]).squeeze() / nspks).reshape([60,60,-1]).numpy()\n",
    "stas0 = ((data.stim[:-lag, ...].T @ Reff[lag:,:]).squeeze() / nspks).reshape([3,60,60,-1]).numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0d569a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "Nrows=int(np.ceil(NCv/4))\n",
    "ss(Nrows,4)\n",
    "for cc in range(NCv):\n",
    "    plt.subplot(Nrows,4, cc+1)\n",
    "    imagesc(stas0[0,:,:, cc])\n",
    "    plt.title(str(cc))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e16bf8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctext = ['Lum', 'L-M', 'S']\n",
    "Nrows=int(np.ceil(NCv/2))\n",
    "ss(Nrows, 6, rh=2)\n",
    "for cc in range(NCv):\n",
    "    for clr in range(3):\n",
    "        plt.subplot(Nrows, 6, 3*cc+clr+1)\n",
    "        imagesc(stas0[clr, :, :, cc])\n",
    "        if clr == 0:\n",
    "            plt.ylabel( \"Cell %d\"%cc)\n",
    "        plt.title(ctext[clr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1362a76f",
   "metadata": {},
   "source": [
    "## GLMs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21cb2f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCv = len(valLP)\n",
    "shifts = np.array([[0,0], [0, 0], [0, 0]]) # if not caring about non-LP things\n",
    "NXglm = UNX\n",
    "\n",
    "LLs0, LLsGLM, XTopt, GLopt = np.zeros([NCv,2]), np.zeros(NCv), np.zeros(NCv), np.zeros(NCv)\n",
    "glms = [None]*NCv\n",
    "driftmods = [None]*NCv\n",
    "Dreg = 0.1\n",
    "\n",
    "lbfgs_pars = utils.create_optimizer_params(\n",
    "    optimizer_type='lbfgs',\n",
    "    tolerance_change=1e-10,\n",
    "    tolerance_grad=1e-10,\n",
    "    batch_size=16000,\n",
    "    max_epochs=3,\n",
    "    max_iter = 200)\n",
    "#lbfgs_pars['num_workers'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f35492b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#set up fits\n",
    "Xreg = 10 # [20]\n",
    "Treg = 1 # [20]\n",
    "L1reg = 1 # [0.5]\n",
    "GLreg = 10.0 # [4.0]\n",
    "\n",
    "# drift network\n",
    "drift_pars1 = NDNLayer.layer_dict( \n",
    "    input_dims=[1,1,1,NA], num_filters=1, bias=False, norm_type=0, NLtype='lin')\n",
    "drift_pars1['reg_vals'] = {'d2t': Dreg, 'bcs':{'d2t':0} } \n",
    "# for stand-alone drift model\n",
    "drift_pars1N = deepcopy(drift_pars1)\n",
    "drift_pars1N['NLtype'] = 'softplus'\n",
    "drift_net =  FFnetwork.ffnet_dict( xstim_n = 'Xdrift', layer_list = [drift_pars1] )\n",
    "\n",
    "# glm net\n",
    "glm_layer = Tlayer.layer_dict( \n",
    "    input_dims=[3,NXglm,NXglm,1], num_filters=1, bias=False, num_lags=num_lags,\n",
    "    NLtype='lin', initialize_center = True)\n",
    "glm_layer['reg_vals'] = {'d2x': Xreg, 'd2t': Treg, 'l1': L1reg, 'glocalx': GLreg,'edge_t':10} \n",
    "stim_net =  FFnetwork.ffnet_dict( xstim_n = 'stim', layer_list = [glm_layer] )\n",
    "\n",
    "# abs layer\n",
    "glm_layer2 = Tlayer.layer_dict( \n",
    "    input_dims = [2,NXglm*3,NXglm,1], num_filters=1, bias=False, num_lags=num_lags, \n",
    "    norm_type=0, NLtype='lin', initialize_center=True) \n",
    "stim_net2 =  FFnetwork.ffnet_dict( xstim_n='stim2', layer_list=[glm_layer2] )\n",
    "\n",
    "# gqm net\n",
    "num_subs = 2\n",
    "gqm_layer = Tlayer.layer_dict( \n",
    "    input_dims=[3,NXglm,NXglm,1], num_filters=num_subs, num_inh=0, bias=False, num_lags=num_lags,\n",
    "    NLtype='square', initialize_center = True)\n",
    "gqm_layer['reg_vals'] = {'d2x': Xreg, 'd2t': Treg, 'l1': L1reg, 'glocalx': GLreg,'edge_t':10} \n",
    "stim_qnet =  FFnetwork.ffnet_dict( xstim_n = 'stim', layer_list = [gqm_layer] )\n",
    "\n",
    "#combine glm\n",
    "comb_layer = NDNLayer.layer_dict(\n",
    "    num_filters = 1, NLtype='softplus', bias=False)\n",
    "comb_layer['weights_initializer'] = 'ones'\n",
    "\n",
    "net_comb = FFnetwork.ffnet_dict( \n",
    "    xstim_n = None, ffnet_n=[0,1],\n",
    "    layer_list = [comb_layer], ffnet_type='add')\n",
    "\n",
    "#combine gqm\n",
    "comb2_layer = ChannelLayer.layer_dict( \n",
    "    num_filters = 1, NLtype='softplus', bias=False)\n",
    "comb2_layer['weights_initializer'] = 'ones'\n",
    "\n",
    "net2_comb = FFnetwork.ffnet_dict( \n",
    "    xstim_n = None, ffnet_n=[0,1,2],\n",
    "    layer_list = [comb2_layer], ffnet_type='normal')\n",
    "net2_comb['layer_list'][0]['bias'] = True\n",
    "\n",
    "glms = [None]*NCv\n",
    "glms_abs = [None]*NCv\n",
    "gqms = [None]*NCv\n",
    "driftmods = [None]*NCv\n",
    "XTopt = np.zeros(NCv) \n",
    "GLopt = np.zeros(NCv) \n",
    "LLsNULL = np.zeros(NCv)\n",
    "LLsR = np.zeros([NCv,4])+1000\n",
    "LL_abs = np.zeros(NCv)\n",
    "LLsQR = np.zeros([NCv,5])+1000\n",
    "\n",
    "rvals = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "rvalsG = [0, 0.001, 0.01, 1, 10, 100, 1000] # glocal\n",
    "\n",
    "# set up stimulus\n",
    "data.assemble_stimulus(top_corner=top_corner, fixdot=0, L=NXglm, time_embed=0, num_lags=num_lags, shifts=-fixshifts )\n",
    "    \n",
    "# for fit model with abs values as well\n",
    "new_stim = torch.zeros([NT, 2, 3*UNX*UNX])\n",
    "new_stim[:,0,:] = deepcopy(data.stim)\n",
    "new_stim[:,1,:] = deepcopy(abs(data.stim))\n",
    "data.add_covariate('stim2', new_stim.reshape([NT, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7d3d258",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# now fit across all models\n",
    "for cc in range(NCv):\n",
    "    #for cc in range(12,14):\n",
    "    #cc=9\n",
    "    data.set_cells([valLP[cc]])\n",
    "\n",
    "    # fit drift network\n",
    "    drift_iter = NDN.NDN( \n",
    "        layer_list = [drift_pars1N], loss_type='poisson')\n",
    "    drift_iter.block_sample=True\n",
    "    drift_iter.networks[0].xstim_n = 'Xdrift'\n",
    "    drift_iter.fit( data, force_dict_training=True, train_inds=None, **lbfgs_pars, verbose=0, version=1)\n",
    "    LLsNULL[cc] = drift_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "    driftmods[cc] = deepcopy(drift_iter)\n",
    "\n",
    "    # first fit GLM\n",
    "    LLs = np.zeros(len(rvals))+1000\n",
    "    for rr in range(len(rvals)):\n",
    "        stim_net['layer_list'][0]['reg_vals']['d2x'] = rvals[rr]\n",
    "        glm_iter = NDN.NDN(ffnet_list = [stim_net, drift_net, net_comb], loss_type='poisson')\n",
    "        glm_iter.block_sample=True\n",
    "        glm_iter.networks[1].layers[0].weight.data[:,0] = deepcopy(\n",
    "            driftmods[cc].networks[0].layers[0].weight.data[:,0])\n",
    "        glm_iter.networks[1].layers[0].set_parameters(val=False)\n",
    "        glm_iter.networks[2].layers[0].set_parameters(val=False,name='weight')\n",
    "\n",
    "        glm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, version=9, verbose=0)\n",
    "        LLi = glm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (rr == 0) or (LLi < np.min(LLs)):\n",
    "            best_model = deepcopy(glm_iter)\n",
    "        LLs[rr] = LLi\n",
    "\n",
    "    bm1 = np.argmin(LLs)\n",
    "    LLsR[cc,0] = LLs[bm1]\n",
    "    XTopt[cc] = rvals[bm1]\n",
    "    glms[cc] = deepcopy(best_model)\n",
    "    print( \"Cell %3d:  d2x-R%d   LLs =\"%(cc, bm1), LLsNULL[cc]-LLsR[cc,0] )\n",
    "\n",
    "    LLs = np.zeros(len(rvals))+LLsR[cc,0]\n",
    "    for rr in range(len(rvals)):\n",
    "        glm_iter = deepcopy(best_model)\n",
    "        glm_iter.networks[0].layers[0].reg.vals['d2t'] = rvals[rr]\n",
    "        glm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = glm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_model = deepcopy(glm_iter)\n",
    "        LLs[rr] = LLi\n",
    "\n",
    "    bm1b = np.argmin(LLs)\n",
    "    LLsR[cc,1] = LLs[bm1b]\n",
    "    GLopt[cc] = rvals[bm1b]\n",
    "    glms[cc] = deepcopy(best_model)\n",
    "    print( \"Cell %3d:  L1  -R%d   LL =%8.5f ->%8.5f\"%(cc, bm1b, LLsNULL[cc]-LLsR[cc,0], LLsNULL[cc]-LLsR[cc,1]) )\n",
    "\n",
    "    LLs = np.zeros(len(rvalsG))+np.min(LLsR[cc,:2])\n",
    "    for rr in range(len(rvalsG)):\n",
    "        glm_iter = deepcopy(best_model)\n",
    "        glm_iter.networks[0].layers[0].reg.vals['glocalx'] = rvalsG[rr]\n",
    "        glm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = glm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_model = deepcopy(glm_iter)\n",
    "        LLs[rr] = LLi\n",
    "\n",
    "    bm2 = np.argmin(LLs)\n",
    "    LLsR[cc,2] = LLs[bm2]\n",
    "    GLopt[cc] = rvals[bm2]\n",
    "    glms[cc] = deepcopy(best_model)\n",
    "    print( \"Cell %3d:  L1  -R%d   LL =%8.5f ->%8.5f\"%(cc, bm2, LLsNULL[cc]-LLsR[cc,0], LLsNULL[cc]-LLsR[cc,2]) )\n",
    "\n",
    "    LLs = np.zeros(len(rvals))+np.min(LLsR[cc,:3])\n",
    "    for rr in range(1,len(rvals)):\n",
    "        glm_iter = deepcopy(best_model)\n",
    "        glm_iter.networks[0].layers[0].reg.vals['l1'] = rvals[rr]\n",
    "        glm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = glm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_model = deepcopy(glm_iter)\n",
    "        LLs[rr] = LLi\n",
    "\n",
    "    bm3 = np.argmin(LLs)\n",
    "    LLsR[cc,3] = LLs[bm3]\n",
    "    GLopt[cc] = rvals[bm3]\n",
    "    glms[cc] = deepcopy(best_model)\n",
    "    print( \"Cell %3d:  Gloc-R%d   LL =%8.5f ->%8.5f\"%(cc, bm3, LLsNULL[cc]-LLsR[cc,0], LLsNULL[cc]-LLsR[cc,3]) )\n",
    "    best_glm = deepcopy(best_model)\n",
    "\n",
    "    # plot GLM filters\n",
    "    w = best_model.get_weights()\n",
    "    utils.subplot_setup(1,num_lags, row_height=4)\n",
    "    for ll in range(num_lags):\n",
    "        plt.subplot(3,num_lags,ll+1)\n",
    "        utils.imagesc(w[0,:,:,ll,0], aspect=1, max=np.max(w))\n",
    "        plt.subplot(3,num_lags,ll+1+num_lags)\n",
    "        utils.imagesc(w[1,:,:,ll,0], aspect=1, max=np.max(w))\n",
    "        plt.subplot(3,num_lags,ll+1+num_lags*2)\n",
    "        utils.imagesc(w[2,:,:,ll,0], aspect=1, max=np.max(w))\n",
    "    plt.show()\n",
    "\n",
    "    ## also fit GLM-abs?\n",
    "    glm_abs = NDN.NDN(ffnet_list = [stim_net2, drift_net, net_comb], loss_type='poisson')\n",
    "    glm_abs.block_sample=True\n",
    "    glm_abs.networks[1].layers[0].weight.data[:,0] = deepcopy(\n",
    "        driftmods[cc].networks[0].layers[0].weight.data[:,0])\n",
    "    # also initialize with previous GLM filter?\n",
    "    glm_abs.networks[2].layers[0].set_parameters(val=False,name='weight')\n",
    "    glm_abs.networks[1].layers[0].set_parameters(val=False)\n",
    "    glm_abs.networks[0].layers[0].reg.vals['d2x'] = rvals[bm1]\n",
    "    glm_abs.networks[0].layers[0].reg.vals['d2t'] = rvals[bm1b]\n",
    "    glm_abs.networks[0].layers[0].reg.vals['glocalx'] = rvals[bm2]\n",
    "    glm_abs.networks[0].layers[0].reg.vals['l1'] = rvalsG[bm3]\n",
    "\n",
    "    glm_abs.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, version=9, verbose=0)\n",
    "    LL_abs[cc] = glm_abs.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "    glms_abs[cc] = deepcopy(glm_abs)\n",
    "    print( \"Cell %3d:  glm+abs   LL =%8.5f ->%8.5f\"%(cc, LLsNULL[cc]-LLsR[cc,0], LLsNULL[cc]-LL_abs[cc]) )\n",
    "\n",
    "    # plot glm_abs\n",
    "\n",
    "    w2 = glms_abs[cc].get_weights()\n",
    "    w2=w2.reshape([2,3,NXglm,NXglm,num_lags,1])\n",
    "\n",
    "    maxlin = np.max(w2[0,:,:,:,:,:])\n",
    "    maxabs = np.max(w2[1,:,:,:,:,:])\n",
    "    maxall = np.max(abs(w2))\n",
    "    utils.ss(2,10, rh=4)\n",
    "    for ll in range(10):\n",
    "        plt.subplot(6,10,ll+1)\n",
    "        utils.imagesc(w2[0,0,:,:,ll,0], aspect=1, max=maxall)\n",
    "\n",
    "        plt.subplot(6,10,ll+11)\n",
    "        utils.imagesc(w2[0,1,:,:,ll,0], aspect=1, max=maxall)\n",
    "\n",
    "        plt.subplot(6,10,ll+21)\n",
    "        utils.imagesc(w2[0,2,:,:,ll,0], aspect=1, max=maxall)\n",
    "\n",
    "        plt.subplot(6,10,ll+31)\n",
    "        utils.imagesc(w2[1,0,:,:,ll,0], aspect=1, max=maxall)\n",
    "\n",
    "        plt.subplot(6,10,ll+41)\n",
    "        utils.imagesc(w2[1,1,:,:,ll,0], aspect=1, max=maxall)\n",
    "\n",
    "        plt.subplot(6,10,ll+51)\n",
    "        utils.imagesc(w2[1,2,:,:,ll,0], aspect=1, max=maxall)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # now fit GQM\n",
    "    LLs = np.zeros(len(rvals))+1000\n",
    "    for rr in range(len(rvals)):\n",
    "        gqm_iter = NDN.NDN(ffnet_list = [stim_net, drift_net, stim_qnet, net2_comb], loss_type='poisson')\n",
    "        gqm_iter.block_sample=True\n",
    "        gqm_iter.networks[3].layers[0].set_parameters(val=False,name='weight')\n",
    "        gqm_iter.networks[1].layers[0].weight.data[:,0] = deepcopy(\n",
    "            driftmods[cc].networks[0].layers[0].weight.data[:,0])\n",
    "        gqm_iter.networks[1].layers[0].set_parameters(val=False)\n",
    "        gqm_iter.networks[0].layers[0].weight.data[:,0] = deepcopy(\n",
    "            best_glm.networks[0].layers[0].weight.data[:,0])\n",
    "        gqm_iter.networks[0].layers[0].reg.vals['d2x'] = rvals[bm1]\n",
    "        gqm_iter.networks[0].layers[0].reg.vals['d2t'] = rvals[bm1b]\n",
    "        gqm_iter.networks[0].layers[0].reg.vals['l1'] = rvals[bm2]\n",
    "        gqm_iter.networks[0].layers[0].reg.vals['glocalx'] = rvalsG[bm3]\n",
    "\n",
    "        gqm_iter.networks[2].layers[0].reg.vals['d2x'] = rvals[rr]\n",
    "        gqm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, version=9, verbose=0)\n",
    "        LLi = gqm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (rr == 0) or (LLi < np.min(LLs)):\n",
    "            best_gqm = deepcopy(gqm_iter)\n",
    "        LLs[rr] = LLi\n",
    "\n",
    "    bmq1 = np.argmin(LLs)\n",
    "    LLsQR[cc,1] = LLs[bmq1]\n",
    "    XTopt[cc] = rvals[bmq1]\n",
    "    gqms[cc] = deepcopy(best_gqm)\n",
    "    print(LLs)\n",
    "    print( \"Cell %3d:  d2x -R%d   LL =%8.5f ->%8.5f\"%(cc, bmq1, LLsNULL[cc]-LLsR[cc,2], LLsNULL[cc]-LLsQR[cc,1]) )\n",
    "\n",
    "    LLs = np.zeros(len(rvals))+np.min(LLsQR[cc,:2])\n",
    "    for rr in range(len(rvals)):\n",
    "        gqm_iter = deepcopy(best_gqm)\n",
    "        gqm_iter.networks[2].layers[0].reg.vals['d2t'] = rvals[rr]\n",
    "        gqm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = gqm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_gqm = deepcopy(gqm_iter)\n",
    "        LLs[rr] = LLi\n",
    "    bmq2 = np.argmin(LLs)\n",
    "    LLsQR[cc,2] = LLs[bmq2]\n",
    "    GLopt[cc] = rvals[bmq2]\n",
    "    gqms[cc] = deepcopy(best_gqm)\n",
    "    print(LLs)\n",
    "    print( \"Cell %3d:  d2t -R%d   LL =%8.5f ->%8.5f\"%(cc, bmq2, LLsNULL[cc]-LLsR[cc,2], LLsNULL[cc]-LLsQR[cc,2]) )\n",
    "\n",
    "    LLs = np.zeros(len(rvalsG))+np.min(LLsQR[cc,:3])\n",
    "    for rr in range(len(rvalsG)):\n",
    "        gqm_iter = deepcopy(best_gqm)\n",
    "        gqm_iter.networks[2].layers[0].reg.vals['glocalx'] = rvalsG[rr]\n",
    "        gqm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = gqm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_gqm = deepcopy(gqm_iter)\n",
    "        LLs[rr] = LLi\n",
    "    bmq3 = np.argmin(LLs)\n",
    "    LLsQR[cc,3] = LLs[bmq3]\n",
    "    GLopt[cc] = rvals[bmq3]\n",
    "    gqms[cc] = deepcopy(best_gqm)\n",
    "    print(LLs)\n",
    "    print( \"Cell %3d:  L1  -R%d   LL =%8.5f ->%8.5f\"%(cc, bmq3, LLsNULL[cc]-LLsR[cc,2], LLsNULL[cc]-LLsQR[cc,3]) )\n",
    "\n",
    "    LLs = np.zeros(len(rvals))+np.min(LLsQR[cc,:4])\n",
    "    for rr in range(len(rvals)):\n",
    "        gqm_iter = deepcopy(best_gqm)\n",
    "        gqm_iter.networks[2].layers[0].reg.vals['l1'] = rvals[rr]\n",
    "        gqm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = gqm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_gqm = deepcopy(gqm_iter)\n",
    "        LLs[rr] = LLi\n",
    "    bmq4 = np.argmin(LLs)\n",
    "    LLsQR[cc,4] = LLs[bmq4]\n",
    "    GLopt[cc] = rvals[bmq4]\n",
    "    gqms[cc] = deepcopy(best_gqm)\n",
    "    print(LLs)\n",
    "    print( \"Cell %3d:  Gloc-R%d   LL =%8.5f ->%8.5f\"%(cc, bmq4, LLsNULL[cc]-LLsR[cc,2], LLsNULL[cc]-LLsQR[cc,4]) )\n",
    "\n",
    "    best_gqm = deepcopy(best_gqm)\n",
    "\n",
    "    w2 = best_gqm.get_weights(ffnet_target=0)\n",
    "    w2=w2.reshape([3,NXglm,NXglm,num_lags])\n",
    "\n",
    "    w3 = best_gqm.get_weights(ffnet_target=2)\n",
    "    w3=w3.reshape([3,NXglm,NXglm,num_lags,num_subs])\n",
    "\n",
    "    maxall = np.max(abs(w3))\n",
    "    utils.ss(3,num_lags, rh=4)\n",
    "    for ll in range( num_lags):\n",
    "        plt.subplot(9,num_lags,ll+1)\n",
    "        utils.imagesc(w2[0,:,:,ll], aspect=1, max=maxall)\n",
    "        plt.subplot(9,num_lags,ll+1+num_lags)\n",
    "        utils.imagesc(w2[1,:,:,ll], aspect=1, max=maxall)\n",
    "        plt.subplot(9,num_lags,ll+1+num_lags*2)\n",
    "        utils.imagesc(w2[2,:,:,ll], aspect=1, max=maxall)\n",
    "        for subs in range(num_subs):\n",
    "            plt.subplot(9,num_lags,ll+1+num_lags*3+(subs*num_lags*3))\n",
    "            utils.imagesc(w3[0,:,:,ll,subs], aspect=1, max=maxall)\n",
    "\n",
    "            plt.subplot(9,num_lags,ll+1+num_lags*4+(subs*num_lags*3))\n",
    "            utils.imagesc(w3[1,:,:,ll,subs], aspect=1, max=maxall)\n",
    "\n",
    "            plt.subplot(9,num_lags,ll+1+num_lags*5+(subs*num_lags*3))\n",
    "            utils.imagesc(w3[2,:,:,ll,subs], aspect=1, max=maxall)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c44765a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# extract measures across cells\n",
    "complex_scores = np.zeros(NCv)\n",
    "lin_corrs = np.zeros(NCv)\n",
    "quad_corrs = np.zeros(NCv)\n",
    "filtwsmat = np.zeros([NCv,3])\n",
    "\n",
    "for cc in range(NCv):\n",
    "    filtwsmat[cc,:]=RU.GQM_filtws(gqms[cc], data, valLP[cc], 0, 2 )\n",
    "    complex_scores[cc], lin_corrs[cc], quad_corrs[cc] = RU.GQM_complexity(gqms[cc], data, valLP[cc], 0, 2 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf950ad5",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_ks = np.zeros([NCv, 3, NXglm, NXglm, num_lags]) \n",
    "glm_abs_ks = np.zeros([NCv, 2, 3*NXglm, NXglm, num_lags]) \n",
    "gqm_ksl = np.zeros([NCv,3, NXglm, NXglm, num_lags])\n",
    "gqm_ksq = np.zeros([NCv,3, NXglm, NXglm, num_lags,num_subs])\n",
    "\n",
    "LLsGLM = LLsNULL-np.min(LLsR,axis=1)\n",
    "LLsGLMabs = LLsNULL-LL_abs\n",
    "LLsGQM = LLsNULL-np.min(LLsQR,axis=1)\n",
    "\n",
    "for cc in range(NCv):\n",
    "    glm_ks[cc,:,:,:,:] = glms[cc].get_weights().squeeze()\n",
    "    glm_abs_ks[cc,:,:,:,:] = glms_abs[cc].get_weights().squeeze()\n",
    "    gqm_ksl[cc,:,:,:,:] = gqms[cc].get_weights(ffnet_target=0).squeeze()\n",
    "    gqm_ksq[cc,:,:,:,:,:] = gqms[cc].get_weights(ffnet_target=2).squeeze()\n",
    "    \n",
    "sio.savemat(dirname2+'LPmodfilts.mat', {\n",
    "    'glm_ks':glm_ks, 'glm_abs_ks':glm_abs_ks, 'gqm_ksl':gqm_ksl, 'gqm_ksq':gqm_ksq,\n",
    "    'valLP':valLP, 'LLsGLM':LLsGLM[:, None],'LLsGLMabs':LLsGLMabs[:, None], 'LLsGQM':LLsGQM[:, None],\n",
    "    'lin_corrs':lin_corrs, 'quad_corrs':quad_corrs, 'complex_scores':complex_scores, 'filtws_gqm':filtwsmat})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be28a929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble population drift terms\n",
    "w0 = driftmods[0].get_weights()\n",
    "NA = w0.shape[0]\n",
    "drift_terms = np.zeros([NA, NCv])\n",
    "for cc in range(NCv):\n",
    "    drift_terms[:, cc] = deepcopy(driftmods[cc].get_weights())[:,0]\n",
    "\n",
    "# Assemble drift population model\n",
    "drift_pars_pop = NDNLayer.layer_dict( \n",
    "    input_dims=[1,1,1,NA], num_filters=NCv, bias=False, norm_type=0, NLtype='softplus')\n",
    "drift_pars_pop['reg_vals'] = {'d2t': Dreg, 'bcs':{'d2t':0} } \n",
    "drift_pop = NDN.NDN( layer_list = [drift_pars_pop], loss_type='poisson')\n",
    "drift_pop.networks[0].xstim_n = 'Xdrift'\n",
    "drift_pop.networks[0].layers[0].weight.data = torch.tensor(drift_terms, dtype=torch.float32)\n",
    "data.cells_out = list(valLP)\n",
    "LLsCHECK = drift_pop.eval_models(data[data.val_blks], null_adjusted=False) \n",
    "print(np.mean(LLsNULL), np.mean(LLsCHECK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dd10ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get array centers\n",
    "RFcenters = np.zeros([NCv,2], dtype=np.int64) - 1\n",
    "RFcols = np.zeros([NCv,1], dtype=np.int64) - 1\n",
    "for cc in range(NCv):\n",
    "    k = glms[cc].get_weights()\n",
    "    pfilt = np.sum(np.std(k, axis=3),axis=0).squeeze() # for colorRFs\n",
    "    x,y = utils.max_multiD( pfilt )\n",
    "    RFcenters[cc, :] = [x,y]\n",
    "\n",
    "plt.plot(RFcenters[:, 0], RFcenters[:, 1],'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c5c7196",
   "metadata": {},
   "source": [
    "### now save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13c89db",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LLsGLM = LLsNULL-np.min(LLsR,axis=1)\n",
    "LLsGLMabs = LLsNULL-np.min(LLsR,axis=1)\n",
    "LLsGQM = LLsNULL-np.min(LLsQR,axis=1)\n",
    "sio.savemat(dirname2+'ModLLsET.mat', {\n",
    "    'LLsNULL':LLsNULL[:,None], 'LLsGLM':LLsGLM[:, None], 'LLsGLMabs':LLsGLMabs[:, None], 'LLsGQM':LLsGQM[:, None],\n",
    "    'drift_terms': drift_terms, 'Dreg': Dreg,\n",
    "    'RFcenters': RFcenters, 'top_corner': top_corner[:, None]})\n",
    "drift_pop.save_model(alt_dirname=dirname_mod, filename='LPdriftmods_pop.pkl')\n",
    "\n",
    "for cc in range(NCv):\n",
    "    glms[cc].save_model(alt_dirname=dirname_mod, filename='LP' + utils.filename_num2str(cc, num_digits=3) + 'glmwET.pkl')\n",
    "    glms_abs[cc].save_model(alt_dirname=dirname_mod, filename='LP' + utils.filename_num2str(cc, num_digits=3) + 'glmabswET.pkl')\n",
    "    gqms[cc].save_model(alt_dirname=dirname_mod, filename='LP' + utils.filename_num2str(cc, num_digits=3) + 'gqmwET.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b204c912",
   "metadata": {},
   "source": [
    "### now get LN correlation model measures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "562b59a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# measure still to translate for gqms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db20c9ab",
   "metadata": {},
   "source": [
    "### now get filter-based activation measures "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57461ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# gqm\n",
    "#net_target = 2\n",
    "#cur_mod = deepcopy(gqms[9])\n",
    "#nf = cur_mod.networks[net_target].layers[0].shape[1]\n",
    "#acts = cur_mod.networks[2].layers[0](data.stim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2968520",
   "metadata": {},
   "outputs": [],
   "source": [
    "#sio.savemat(dirname2+'GLMT_measLP_ET.mat', {\n",
    "#    'glm_ks':glm_ks, 'valLP':valLP, 'LLsGLM':LLsGLM[:, None],\n",
    "#    'RFcenters': RFcenters, 'Colws': Gconv_cws, 'RFareas': RF_areas, 'RFareas_col':RF_areas_col,\n",
    "#    'RFmaps': Gconv_space_all, 'RFmaps_col': Gconv_space_col_all, \n",
    "#    'RFbs': Gconv_space_shuff_all, 'RFbs_col': Gconv_space_col_shuff_all, \n",
    "#    'Contours':contours_all, 'Contours_col':contours_col_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-louis",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78505b95",
   "metadata": {},
   "source": [
    "## Now fit Utah array models again to check ET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef40ab32",
   "metadata": {},
   "outputs": [],
   "source": [
    "UTloc = np.array([930, 515], dtype=np.int64)\n",
    "\n",
    "data.assemble_stimulus(top_corner=UTloc, fixdot=0, L=UNX, time_embed=0, shifts=-fixshifts)\n",
    "Reff = torch.mul(data.robs[:, valUT], data.dfs[:, valUT])\n",
    "nspks = torch.sum(Reff, axis=0)\n",
    "\n",
    "data.draw_stim_locations( top_corner=UTloc, L=UNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38733a6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate STAS\n",
    "lag = 4\n",
    "stasC = ((data.stim[:-lag, ...].T @ Reff[lag:,:]).squeeze() / nspks).reshape([3, UNX,UNX,-1]).numpy()\n",
    "stasC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41519ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctext = ['Lum', 'L-M', 'S']\n",
    "ss(86, 6, rh=2)\n",
    "for cc in range(NCUT):\n",
    "    for clr in range(3):\n",
    "        plt.subplot(86, 6, 3*cc+clr+1)\n",
    "        imagesc(stasC[clr, :, :, cc])\n",
    "        if clr == 0:\n",
    "            plt.ylabel( \"Cell %d\"%cc)\n",
    "        plt.title(ctext[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dde1d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCv=NCUT\n",
    "\n",
    "NXglm = UNX\n",
    "#num_lags = 8\n",
    "\n",
    "LLs0, LLsGLM, XTopt, GLopt = np.zeros([NCv,2]), np.zeros(NCv), np.zeros(NCv), np.zeros(NCv)\n",
    "glms = [None]*NCv\n",
    "driftmods = [None]*NCv\n",
    "Dreg = 0.1\n",
    "\n",
    "lbfgs_pars = utils.create_optimizer_params(\n",
    "    optimizer_type='lbfgs',\n",
    "    tolerance_change=1e-10,\n",
    "    tolerance_grad=1e-10,\n",
    "    batch_size=16000,\n",
    "    max_epochs=3,\n",
    "    max_iter = 200)\n",
    "#lbfgs_pars['num_workers'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d87436",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set up fits\n",
    "Xreg = 10 # [20]\n",
    "Treg = 1 # [20]\n",
    "L1reg = 1 # [0.5]\n",
    "GLreg = 10.0 # [4.0]\n",
    "\n",
    "# drift network\n",
    "drift_pars1 = NDNLayer.layer_dict( \n",
    "    input_dims=[1,1,1,NA], num_filters=1, bias=False, norm_type=0, NLtype='lin')\n",
    "drift_pars1['reg_vals'] = {'d2t': Dreg, 'bcs':{'d2t':0} } \n",
    "# for stand-alone drift model\n",
    "drift_pars1N = deepcopy(drift_pars1)\n",
    "drift_pars1N['NLtype'] = 'softplus'\n",
    "drift_net =  FFnetwork.ffnet_dict( xstim_n = 'Xdrift', layer_list = [drift_pars1] )\n",
    "\n",
    "# glm net\n",
    "glm_layer = Tlayer.layer_dict( \n",
    "    input_dims=[3,NXglm,NXglm,1], num_filters=1, bias=False, num_lags=num_lags,\n",
    "    NLtype='lin', initialize_center = True)\n",
    "glm_layer['reg_vals'] = {'d2x': Xreg, 'd2t': Treg, 'l1': L1reg, 'glocalx': GLreg,'edge_t':10} \n",
    "stim_net =  FFnetwork.ffnet_dict( xstim_n = 'stim', layer_list = [glm_layer] )\n",
    "\n",
    "# abs layer\n",
    "glm_layer2 = Tlayer.layer_dict( \n",
    "    input_dims = [2,NXglm*3,NXglm,1], num_filters=1, bias=False, num_lags=num_lags, \n",
    "    norm_type=0, NLtype='lin', initialize_center=True) \n",
    "stim_net2 =  FFnetwork.ffnet_dict( xstim_n='stim2', layer_list=[glm_layer2] )\n",
    "\n",
    "# gqm net\n",
    "num_subs = 2\n",
    "gqm_layer = Tlayer.layer_dict( \n",
    "    input_dims=[3,NXglm,NXglm,1], num_filters=num_subs, num_inh=0, bias=False, num_lags=num_lags,\n",
    "    NLtype='square', initialize_center = True)\n",
    "gqm_layer['reg_vals'] = {'d2x': Xreg, 'd2t': Treg, 'l1': L1reg, 'glocalx': GLreg,'edge_t':10} \n",
    "stim_qnet =  FFnetwork.ffnet_dict( xstim_n = 'stim', layer_list = [gqm_layer] )\n",
    "\n",
    "#combine glm\n",
    "comb_layer = NDNLayer.layer_dict(\n",
    "    num_filters = 1, NLtype='softplus', bias=False)\n",
    "comb_layer['weights_initializer'] = 'ones'\n",
    "\n",
    "net_comb = FFnetwork.ffnet_dict( \n",
    "    xstim_n = None, ffnet_n=[0,1],\n",
    "    layer_list = [comb_layer], ffnet_type='add')\n",
    "\n",
    "#combine gqm\n",
    "comb2_layer = ChannelLayer.layer_dict( \n",
    "    num_filters = 1, NLtype='softplus', bias=False)\n",
    "comb2_layer['weights_initializer'] = 'ones'\n",
    "\n",
    "net2_comb = FFnetwork.ffnet_dict( \n",
    "    xstim_n = None, ffnet_n=[0,1,2],\n",
    "    layer_list = [comb2_layer], ffnet_type='normal')\n",
    "net2_comb['layer_list'][0]['bias'] = True\n",
    "\n",
    "glms = [None]*NCv\n",
    "glms_abs = [None]*NCv\n",
    "gqms = [None]*NCv\n",
    "driftmods = [None]*NCv\n",
    "XTopt = np.zeros(NCv) \n",
    "GLopt = np.zeros(NCv) \n",
    "LLsNULL = np.zeros(NCv)\n",
    "LLsR = np.zeros([NCv,4])+1000\n",
    "LL_abs = np.zeros(NCv)\n",
    "LLsQR = np.zeros([NCv,5])+1000\n",
    "\n",
    "rvals = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "rvalsG = [0, 0.001, 0.01, 1, 10, 100, 1000] # glocal\n",
    "\n",
    "# set up stimulus\n",
    "data.assemble_stimulus(top_corner=UTloc, fixdot=0, L=NXglm, time_embed=0, num_lags=num_lags, shifts=-fixshifts )\n",
    "    \n",
    "# for fit model with abs values as well\n",
    "new_stim = torch.zeros([NT, 2, 3*UNX*UNX])\n",
    "new_stim[:,0,:] = deepcopy(data.stim)\n",
    "new_stim[:,1,:] = deepcopy(abs(data.stim))\n",
    "data.add_covariate('stim2', new_stim.reshape([NT, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cfe979",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now fit across all models\n",
    "for cc in range(NCv):\n",
    "    #for cc in range(12,14):\n",
    "    #cc=9\n",
    "    data.set_cells([valUT[cc]])\n",
    "\n",
    "    # fit drift network\n",
    "    drift_iter = NDN.NDN( \n",
    "        layer_list = [drift_pars1N], loss_type='poisson')\n",
    "    drift_iter.block_sample=True\n",
    "    drift_iter.networks[0].xstim_n = 'Xdrift'\n",
    "    drift_iter.fit( data, force_dict_training=True, train_inds=None, **lbfgs_pars, verbose=0, version=1)\n",
    "    LLsNULL[cc] = drift_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "    driftmods[cc] = deepcopy(drift_iter)\n",
    "\n",
    "    # first fit GLM\n",
    "    LLs = np.zeros(len(rvals))+1000\n",
    "    for rr in range(len(rvals)):\n",
    "        stim_net['layer_list'][0]['reg_vals']['d2x'] = rvals[rr]\n",
    "        glm_iter = NDN.NDN(ffnet_list = [stim_net, drift_net, net_comb], loss_type='poisson')\n",
    "        glm_iter.block_sample=True\n",
    "        glm_iter.networks[1].layers[0].weight.data[:,0] = deepcopy(\n",
    "            driftmods[cc].networks[0].layers[0].weight.data[:,0])\n",
    "        glm_iter.networks[1].layers[0].set_parameters(val=False)\n",
    "        glm_iter.networks[2].layers[0].set_parameters(val=False,name='weight')\n",
    "\n",
    "        glm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, version=9, verbose=0)\n",
    "        LLi = glm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (rr == 0) or (LLi < np.min(LLs)):\n",
    "            best_model = deepcopy(glm_iter)\n",
    "        LLs[rr] = LLi\n",
    "\n",
    "    bm1 = np.argmin(LLs)\n",
    "    LLsR[cc,0] = LLs[bm1]\n",
    "    XTopt[cc] = rvals[bm1]\n",
    "    glms[cc] = deepcopy(best_model)\n",
    "    print( \"Cell %3d:  d2x-R%d   LLs =\"%(cc, bm1), LLsNULL[cc]-LLsR[cc,0] )\n",
    "\n",
    "    LLs = np.zeros(len(rvals))+LLsR[cc,0]\n",
    "    for rr in range(len(rvals)):\n",
    "        glm_iter = deepcopy(best_model)\n",
    "        glm_iter.networks[0].layers[0].reg.vals['d2t'] = rvals[rr]\n",
    "        glm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = glm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_model = deepcopy(glm_iter)\n",
    "        LLs[rr] = LLi\n",
    "\n",
    "    bm1b = np.argmin(LLs)\n",
    "    LLsR[cc,1] = LLs[bm1b]\n",
    "    GLopt[cc] = rvals[bm1b]\n",
    "    glms[cc] = deepcopy(best_model)\n",
    "    print( \"Cell %3d:  L1  -R%d   LL =%8.5f ->%8.5f\"%(cc, bm1b, LLsNULL[cc]-LLsR[cc,0], LLsNULL[cc]-LLsR[cc,1]) )\n",
    "\n",
    "    LLs = np.zeros(len(rvalsG))+np.min(LLsR[cc,:2])\n",
    "    for rr in range(len(rvalsG)):\n",
    "        glm_iter = deepcopy(best_model)\n",
    "        glm_iter.networks[0].layers[0].reg.vals['glocalx'] = rvalsG[rr]\n",
    "        glm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = glm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_model = deepcopy(glm_iter)\n",
    "        LLs[rr] = LLi\n",
    "\n",
    "    bm2 = np.argmin(LLs)\n",
    "    LLsR[cc,2] = LLs[bm2]\n",
    "    GLopt[cc] = rvals[bm2]\n",
    "    glms[cc] = deepcopy(best_model)\n",
    "    print( \"Cell %3d:  Gloc-R%d   LL =%8.5f ->%8.5f\"%(cc, bm2, LLsNULL[cc]-LLsR[cc,0], LLsNULL[cc]-LLsR[cc,2]) )\n",
    "\n",
    "    LLs = np.zeros(len(rvals))+np.min(LLsR[cc,:3])\n",
    "    for rr in range(1,len(rvals)):\n",
    "        glm_iter = deepcopy(best_model)\n",
    "        glm_iter.networks[0].layers[0].reg.vals['l1'] = rvals[rr]\n",
    "        glm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = glm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_model = deepcopy(glm_iter)\n",
    "        LLs[rr] = LLi\n",
    "\n",
    "    bm3 = np.argmin(LLs)\n",
    "    LLsR[cc,3] = LLs[bm3]\n",
    "    GLopt[cc] = rvals[bm3]\n",
    "    glms[cc] = deepcopy(best_model)\n",
    "    print( \"Cell %3d:  L1-R%d   LL =%8.5f ->%8.5f\"%(cc, bm3, LLsNULL[cc]-LLsR[cc,0], LLsNULL[cc]-LLsR[cc,3]) )\n",
    "    best_glm = deepcopy(best_model)\n",
    "\n",
    "    # plot GLM filters\n",
    "    w = best_model.get_weights()\n",
    "    utils.subplot_setup(1,num_lags, row_height=4)\n",
    "    for ll in range(num_lags):\n",
    "        plt.subplot(3,num_lags,ll+1)\n",
    "        utils.imagesc(w[0,:,:,ll,0], aspect=1, max=np.max(w))\n",
    "        plt.subplot(3,num_lags,ll+1+num_lags)\n",
    "        utils.imagesc(w[1,:,:,ll,0], aspect=1, max=np.max(w))\n",
    "        plt.subplot(3,num_lags,ll+1+num_lags*2)\n",
    "        utils.imagesc(w[2,:,:,ll,0], aspect=1, max=np.max(w))\n",
    "    plt.show()\n",
    "\n",
    "    ## also fit GLM-abs?\n",
    "    glm_abs = NDN.NDN(ffnet_list = [stim_net2, drift_net, net_comb], loss_type='poisson')\n",
    "    glm_abs.block_sample=True\n",
    "    glm_abs.networks[1].layers[0].weight.data[:,0] = deepcopy(\n",
    "        driftmods[cc].networks[0].layers[0].weight.data[:,0])\n",
    "    # also initialize with previous GLM filter?\n",
    "    glm_abs.networks[2].layers[0].set_parameters(val=False,name='weight')\n",
    "    glm_abs.networks[1].layers[0].set_parameters(val=False)\n",
    "    glm_abs.networks[0].layers[0].reg.vals['d2x'] = rvals[bm1]\n",
    "    glm_abs.networks[0].layers[0].reg.vals['d2t'] = rvals[bm1b]\n",
    "    glm_abs.networks[0].layers[0].reg.vals['glocalx'] = rvalsG[bm2]\n",
    "    glm_abs.networks[0].layers[0].reg.vals['l1'] = rvals[bm3]\n",
    "\n",
    "    glm_abs.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, version=9, verbose=0)\n",
    "    LL_abs[cc] = glm_abs.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "    glms_abs[cc] = deepcopy(glm_abs)\n",
    "    print( \"Cell %3d:  glm+abs   LL =%8.5f ->%8.5f\"%(cc, LLsNULL[cc]-LLsR[cc,0], LLsNULL[cc]-LL_abs[cc]) )\n",
    "\n",
    "    # plot glm_abs\n",
    "\n",
    "    w2 = glms_abs[cc].get_weights()\n",
    "    w2=w2.reshape([2,3,NXglm,NXglm,num_lags,1])\n",
    "\n",
    "    maxlin = np.max(w2[0,:,:,:,:,:])\n",
    "    maxabs = np.max(w2[1,:,:,:,:,:])\n",
    "    maxall = np.max(abs(w2))\n",
    "    utils.ss(2,10, rh=4)\n",
    "    for ll in range(10):\n",
    "        plt.subplot(6,10,ll+1)\n",
    "        utils.imagesc(w2[0,0,:,:,ll,0], aspect=1, max=maxall)\n",
    "\n",
    "        plt.subplot(6,10,ll+11)\n",
    "        utils.imagesc(w2[0,1,:,:,ll,0], aspect=1, max=maxall)\n",
    "\n",
    "        plt.subplot(6,10,ll+21)\n",
    "        utils.imagesc(w2[0,2,:,:,ll,0], aspect=1, max=maxall)\n",
    "\n",
    "        plt.subplot(6,10,ll+31)\n",
    "        utils.imagesc(w2[1,0,:,:,ll,0], aspect=1, max=maxall)\n",
    "\n",
    "        plt.subplot(6,10,ll+41)\n",
    "        utils.imagesc(w2[1,1,:,:,ll,0], aspect=1, max=maxall)\n",
    "\n",
    "        plt.subplot(6,10,ll+51)\n",
    "        utils.imagesc(w2[1,2,:,:,ll,0], aspect=1, max=maxall)\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "    # now fit GQM\n",
    "    LLs = np.zeros(len(rvals))+1000\n",
    "#    LLsQR[cc,0]=np.min(LLsR[cc,:])\n",
    "    for rr in range(len(rvals)):\n",
    "        gqm_iter = NDN.NDN(ffnet_list = [stim_net, drift_net, stim_qnet, net2_comb], loss_type='poisson')\n",
    "        gqm_iter.block_sample=True\n",
    "        gqm_iter.networks[3].layers[0].set_parameters(val=False,name='weight')\n",
    "        gqm_iter.networks[1].layers[0].weight.data[:,0] = deepcopy(\n",
    "            driftmods[cc].networks[0].layers[0].weight.data[:,0])\n",
    "        gqm_iter.networks[1].layers[0].set_parameters(val=False)\n",
    "        gqm_iter.networks[0].layers[0].weight.data[:,0] = deepcopy(\n",
    "            best_glm.networks[0].layers[0].weight.data[:,0])\n",
    "        gqm_iter.networks[0].layers[0].reg.vals['d2x'] = rvals[bm1]\n",
    "        gqm_iter.networks[0].layers[0].reg.vals['d2t'] = rvals[bm1b]\n",
    "        gqm_iter.networks[0].layers[0].reg.vals['glocalx'] = rvalsG[bm2]\n",
    "        gqm_iter.networks[0].layers[0].reg.vals['l1'] = rvals[bm3]\n",
    "\n",
    "        gqm_iter.networks[2].layers[0].reg.vals['d2x'] = rvals[rr]\n",
    "        gqm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, version=9, verbose=0)\n",
    "        LLi = gqm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (rr == 0) or (LLi < np.min(LLs)):\n",
    "            best_gqm = deepcopy(gqm_iter)\n",
    "        LLs[rr] = LLi\n",
    "\n",
    "    bmq1 = np.argmin(LLs)\n",
    "    LLsQR[cc,1] = LLs[bmq1]\n",
    "    XTopt[cc] = rvals[bmq1]\n",
    "    gqms[cc] = deepcopy(best_gqm)\n",
    "    print(LLs)\n",
    "    print( \"Cell %3d:  d2x -R%d   LL =%8.5f ->%8.5f\"%(cc, bmq1, LLsNULL[cc]-LLsR[cc,2], LLsNULL[cc]-LLsQR[cc,1]) )\n",
    "\n",
    "    LLs = np.zeros(len(rvals))+np.min(LLsQR[cc,:2])\n",
    "    for rr in range(len(rvals)):\n",
    "        gqm_iter = deepcopy(best_gqm)\n",
    "        gqm_iter.networks[2].layers[0].reg.vals['d2t'] = rvals[rr]\n",
    "        gqm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = gqm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_gqm = deepcopy(gqm_iter)\n",
    "        LLs[rr] = LLi\n",
    "    bmq2 = np.argmin(LLs)\n",
    "    LLsQR[cc,2] = LLs[bmq2]\n",
    "    GLopt[cc] = rvals[bmq2]\n",
    "    gqms[cc] = deepcopy(best_gqm)\n",
    "    print(LLs)\n",
    "    print( \"Cell %3d:  d2t -R%d   LL =%8.5f ->%8.5f\"%(cc, bmq2, LLsNULL[cc]-LLsR[cc,2], LLsNULL[cc]-LLsQR[cc,2]) )\n",
    "\n",
    "    LLs = np.zeros(len(rvalsG))+np.min(LLsQR[cc,:3])\n",
    "    for rr in range(len(rvalsG)):\n",
    "        gqm_iter = deepcopy(best_gqm)\n",
    "        gqm_iter.networks[2].layers[0].reg.vals['glocalx'] = rvalsG[rr]\n",
    "        gqm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = gqm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_gqm = deepcopy(gqm_iter)\n",
    "        LLs[rr] = LLi\n",
    "    bmq3 = np.argmin(LLs)\n",
    "    LLsQR[cc,3] = LLs[bmq3]\n",
    "    GLopt[cc] = rvals[bmq3]\n",
    "    gqms[cc] = deepcopy(best_gqm)\n",
    "    print(LLs)\n",
    "    print( \"Cell %3d:  L1  -R%d   LL =%8.5f ->%8.5f\"%(cc, bmq3, LLsNULL[cc]-LLsR[cc,2], LLsNULL[cc]-LLsQR[cc,3]) )\n",
    "\n",
    "    LLs = np.zeros(len(rvals))+np.min(LLsQR[cc,:4])\n",
    "    for rr in range(len(rvals)):\n",
    "        gqm_iter = deepcopy(best_gqm)\n",
    "        gqm_iter.networks[2].layers[0].reg.vals['l1'] = rvals[rr]\n",
    "        gqm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = gqm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_gqm = deepcopy(gqm_iter)\n",
    "        LLs[rr] = LLi\n",
    "    bmq4 = np.argmin(LLs)\n",
    "    LLsQR[cc,4] = LLs[bmq4]\n",
    "    GLopt[cc] = rvals[bmq4]\n",
    "    gqms[cc] = deepcopy(best_gqm)\n",
    "    print(LLs)\n",
    "    print( \"Cell %3d:  Gloc-R%d   LL =%8.5f ->%8.5f\"%(cc, bmq4, LLsNULL[cc]-LLsR[cc,2], LLsNULL[cc]-LLsQR[cc,4]) )\n",
    "\n",
    "    best_gqm = deepcopy(best_gqm)\n",
    "\n",
    "    w2 = best_gqm.get_weights(ffnet_target=0)\n",
    "    w2=w2.reshape([3,NXglm,NXglm,num_lags])\n",
    "\n",
    "    w3 = best_gqm.get_weights(ffnet_target=2)\n",
    "    w3=w3.reshape([3,NXglm,NXglm,num_lags,num_subs])\n",
    "\n",
    "    maxall = np.max(abs(w3))\n",
    "    utils.ss(3,num_lags, rh=4)\n",
    "    for ll in range( num_lags):\n",
    "        plt.subplot(9,num_lags,ll+1)\n",
    "        utils.imagesc(w2[0,:,:,ll], aspect=1, max=maxall)\n",
    "        plt.subplot(9,num_lags,ll+1+num_lags)\n",
    "        utils.imagesc(w2[1,:,:,ll], aspect=1, max=maxall)\n",
    "        plt.subplot(9,num_lags,ll+1+num_lags*2)\n",
    "        utils.imagesc(w2[2,:,:,ll], aspect=1, max=maxall)\n",
    "        for subs in range(num_subs):\n",
    "            plt.subplot(9,num_lags,ll+1+num_lags*3+(subs*num_lags*3))\n",
    "            utils.imagesc(w3[0,:,:,ll,subs], aspect=1, max=maxall)\n",
    "\n",
    "            plt.subplot(9,num_lags,ll+1+num_lags*4+(subs*num_lags*3))\n",
    "            utils.imagesc(w3[1,:,:,ll,subs], aspect=1, max=maxall)\n",
    "\n",
    "            plt.subplot(9,num_lags,ll+1+num_lags*5+(subs*num_lags*3))\n",
    "            utils.imagesc(w3[2,:,:,ll,subs], aspect=1, max=maxall)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfd523b",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_ks = np.zeros([NCv, 3, NXglm, NXglm, num_lags]) \n",
    "glm_abs_ks = np.zeros([NCv, 2, 3*NXglm, NXglm, num_lags]) \n",
    "gqm_ksl = np.zeros([NCv,3, NXglm, NXglm, num_lags])\n",
    "gqm_ksq = np.zeros([NCv,3, NXglm, NXglm, num_lags,num_subs])\n",
    "\n",
    "LLsGLM = LLsNULL-np.min(LLsR,axis=1)\n",
    "LLsGLMabs = LLsNULL-LL_abs\n",
    "LLsGQM = LLsNULL-np.min(LLsQR,axis=1)\n",
    "\n",
    "for cc in range(NCv):\n",
    "    glm_ks[cc,:,:,:,:] = glms[cc].get_weights().squeeze()\n",
    "    glm_abs_ks[cc,:,:,:,:] = glms_abs[cc].get_weights().squeeze()\n",
    "    gqm_ksl[cc,:,:,:,:] = gqms[cc].get_weights(ffnet_target=0).squeeze()\n",
    "    gqm_ksq[cc,:,:,:,:,:] = gqms[cc].get_weights(ffnet_target=2).squeeze()\n",
    "    \n",
    "sio.savemat(dirname2+'UTmodfilts.mat', {\n",
    "    'glm_ks':glm_ks, 'glm_abs_ks':glm_abs_ks, 'gqm_ksl':gqm_ksl, 'gqm_ksq':gqm_ksq,\n",
    "    'valUT':valUT, 'LLsGLM':LLsGLM[:, None],'LLsGLMabs':LLsGLMabs[:, None], 'LLsGQM':LLsGQM[:, None]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be6aef9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble population drift terms\n",
    "w0 = driftmods[0].get_weights()\n",
    "NA = w0.shape[0]\n",
    "drift_terms = np.zeros([NA, NCv])\n",
    "for cc in range(NCv):\n",
    "    drift_terms[:, cc] = deepcopy(driftmods[cc].get_weights())[:,0]\n",
    "\n",
    "# Assemble drift population model\n",
    "drift_pars_pop = NDNLayer.layer_dict( \n",
    "    input_dims=[1,1,1,NA], num_filters=NCv, bias=False, norm_type=0, NLtype='softplus')\n",
    "drift_pars_pop['reg_vals'] = {'d2t': Dreg, 'bcs':{'d2t':0} } \n",
    "drift_pop = NDN.NDN( layer_list = [drift_pars_pop], loss_type='poisson')\n",
    "drift_pop.networks[0].xstim_n = 'Xdrift'\n",
    "drift_pop.networks[0].layers[0].weight.data = torch.tensor(drift_terms, dtype=torch.float32)\n",
    "data.cells_out = list(valLP)\n",
    "LLsCHECK = drift_pop.eval_models(data[data.val_blks], null_adjusted=False) \n",
    "print(np.mean(LLsNULL), np.mean(LLsCHECK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21873ac3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Get array centers\n",
    "RFcenters = np.zeros([NCv,2], dtype=np.int64) - 1\n",
    "RFcols = np.zeros([NCv,1], dtype=np.int64) - 1\n",
    "for cc in range(NCv):\n",
    "    k = glms[cc].get_weights()\n",
    "    pfilt = np.sum(np.std(k, axis=3),axis=0).squeeze() # for colorRFs\n",
    "    x,y = utils.max_multiD( pfilt )\n",
    "    RFcenters[cc, :] = [x,y]\n",
    "\n",
    "plt.plot(RFcenters[:, 0], RFcenters[:, 1],'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00fd11f4",
   "metadata": {},
   "source": [
    "### now save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37f974ea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "LLsGLM = LLsNULL-np.min(LLsR,axis=1)\n",
    "LLsGLMabs = LLsNULL-np.min(LLsR,axis=1)\n",
    "LLsGQM = LLsNULL-np.min(LLsQR,axis=1)\n",
    "sio.savemat(dirname2+'ModLLsET_UT.mat', {\n",
    "    'LLsNULL':LLsNULL[:,None], 'LLsGLM':LLsGLM[:, None], 'LLsGLMabs':LLsGLMabs[:, None], 'LLsGQM':LLsGQM[:, None],\n",
    "    'drift_terms': drift_terms, 'Dreg': Dreg,\n",
    "    'RFcenters': RFcenters, 'top_corner': top_corner[:, None]})\n",
    "drift_pop.save_model(alt_dirname=dirname_mod, filename='UTdriftmods_pop.pkl')\n",
    "\n",
    "for cc in range(NCv):\n",
    "    glms[cc].save_model(alt_dirname=dirname_mod, filename='UT' + utils.filename_num2str(cc, num_digits=3) + 'glmwET.pkl')\n",
    "    glms_abs[cc].save_model(alt_dirname=dirname_mod, filename='UT' + utils.filename_num2str(cc, num_digits=3) + 'glmabswET.pkl')\n",
    "    gqms[cc].save_model(alt_dirname=dirname_mod, filename='UT' + utils.filename_num2str(cc, num_digits=3) + 'gqmwET.pkl')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6307e60",
   "metadata": {},
   "source": [
    "### TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "645d6270",
   "metadata": {},
   "outputs": [],
   "source": [
    "crash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "231017b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble population drift terms\n",
    "w0 = driftmods[0].get_weights()\n",
    "NA = w0.shape[0]\n",
    "drift_terms = np.zeros([NA, NCv])\n",
    "for cc in range(NCv):\n",
    "    drift_terms[:, cc] = deepcopy(driftmods[cc].get_weights())[:,0]\n",
    "\n",
    "# Assemble drift population model\n",
    "drift_pars_pop = NDNLayer.layer_dict( \n",
    "    input_dims=[1,1,1,NA], num_filters=NCv, bias=False, norm_type=0, NLtype='softplus')\n",
    "drift_pars_pop['reg_vals'] = {'d2t': Dreg, 'bcs':{'d2t':0} } \n",
    "drift_pop = NDN.NDN( layer_list=[drift_pars_pop], loss_type='poisson')\n",
    "drift_pop.networks[0].xstim_n = 'Xdrift'\n",
    "drift_pop.networks[0].layers[0].weight.data = torch.tensor(drift_terms, dtype=torch.float32)\n",
    "data.set_cells(valUT)\n",
    "LLsCHECK = drift_pop.eval_models(data[data.val_blks], null_adjusted=False) \n",
    "print(np.mean(LLsNULL), np.mean(LLsCHECK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84893cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get array centers\n",
    "RFcenters = np.zeros([NCv,2], dtype=np.int64) - 1\n",
    "for cc in range(NCv):\n",
    "    k = glms[cc].get_weights()\n",
    "    pfilt = np.sum(np.std(k, axis=3),axis=0).squeeze() # for colorRFs\n",
    "    x,y = utils.max_multiD( pfilt )\n",
    "    RFcenters[cc, :] = [x,y]   \n",
    "#    pfilt = np.std(k, axis=2).squeeze()\n",
    "#    x,y, snrs[cc] = CU.RFstd_evaluate( pfilt )\n",
    "#    RFcenters[cc, :] = [x,y]\n",
    "\n",
    "plt.plot(RFcenters[:, 0], RFcenters[:, 1],'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebf37a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full GLM model like full drift model\n",
    "glm_layer = NDNLayer.layer_dict( \n",
    "    input_dims=[3, NXglm, NXglm, num_lags], num_filters=NCv, bias=False, NLtype='lin', initialize_center=True)\n",
    "glm_layer['reg_vals'] = {'d2xt': XTreg, 'l1': L1reg, 'glocalx': GLreg,'edge_t':100} \n",
    "\n",
    "stim_net =  FFnetwork.ffnet_dict( xstim_n='stim', layer_list=[glm_layer] )\n",
    "\n",
    "drift_pars = NDNLayer.layer_dict( \n",
    "    input_dims=[1,1,1,NA], num_filters=NCv, bias=False, norm_type=0, NLtype='lin')\n",
    "drift_pars['reg_vals'] = {'d2t': Dreg, 'bcs':{'d2t':0} } \n",
    "\n",
    "drift_net =  FFnetwork.ffnet_dict( xstim_n = 'Xdrift', layer_list=[drift_pars] )\n",
    "\n",
    "comb_layer = ChannelLayer.layer_dict( num_filters=NCv, NLtype='softplus', bias=True )\n",
    "comb_layer['weights_initializer'] = 'ones'\n",
    "\n",
    "net_comb = FFnetwork.ffnet_dict( \n",
    "    xstim_n = None, ffnet_n=[0,1],\n",
    "    layer_list = [comb_layer], ffnet_type='add')\n",
    "\n",
    "glm_all = NDN.NDN(  \n",
    "            ffnet_list = [stim_net, drift_net, net_comb], loss_type='poisson')\n",
    "\n",
    "for cc in range(NCv):\n",
    "    glm_all.networks[0].layers[0].weight.data[:, cc] = deepcopy(glms[cc].networks[0].layers[0].weight.data[:,0])\n",
    "    glm_all.networks[1].layers[0].weight.data[:, cc] = deepcopy(glms[cc].networks[1].layers[0].weight.data[:,0])\n",
    "    glm_all.networks[-1].layers[0].bias.data[cc] = deepcopy(glms[cc].networks[-1].layers[0].bias.data)\n",
    "\n",
    "data.set_cells(valUT)\n",
    "LLsCHECK = glm_all.eval_models(data[data.val_blks], null_adjusted=False) \n",
    "print(np.mean(LLsNULL-LLsR[:,1]), np.mean(LLsNULL-LLsCHECK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a2cf2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLsGLM = LLsNULL-LLsR[:,1]\n",
    "sio.savemat(dirname2+'LLsGLMTwET_UT.mat', {\n",
    "    'LLsNULL':LLsNULL[:,None], 'LLsGLM':LLsGLM[:, None], 'drift_terms': drift_terms, 'Dreg': Dreg,\n",
    "    'RFcenters': RFcenters, 'top_corner': UTloc})\n",
    "drift_pop.save_model(alt_dirname=dirname_mod, filename='ETdriftmods_pop.pkl')\n",
    "glm_all.save_model(alt_dirname=dirname_mod, filename='ETglms_pop.pkl')\n",
    "\n",
    "for cc in range(NCv):\n",
    "    glms[cc].save_model(\n",
    "        alt_dirname=dirname_mod, filename='UTglmT' + utils.filename_num2str(cc, num_digits=3) + 'wET_iter0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8627833c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure color weights\n",
    "Gconv_cws = np.zeros([NCv,3])\n",
    "Gconv_space_all = np.zeros([NCv,NXglm,NXglm])\n",
    "Gconv_space_col_all = np.zeros([NCv,3,NXglm,NXglm])\n",
    "Gconv_space_shuff_all = np.zeros([NCv,NXglm,NXglm])\n",
    "Gconv_space_col_shuff_all = np.zeros([NCv,3,NXglm,NXglm])\n",
    "\n",
    "RF_areas = np.zeros([NCv])\n",
    "RF_areas_col = np.zeros([NCv,3])\n",
    "contours_all = []\n",
    "contours_col_all = [] # dtype=object\n",
    "\n",
    "for cc in range(NCv):\n",
    "    data.cells_out = [valUT[cc]]\n",
    "    Gconv_cws[cc,0], Gconv_cws[cc,1], Gconv_cws[cc,2] = RU.get_Gconv_colws(glms[cc], data, valUT[cc])\n",
    "    [Gconv_space_all[cc,:,:], Gconv_space_col_all[cc,:,:,:]] = RU.get_Gconv_RFmap(glms[cc], data, valUT[cc], drift_mod=driftmods[cc])\n",
    "    [Gconv_space_shuff_all[cc,:,:], Gconv_space_col_shuff_all[cc,:,:,:]] = RU.get_Gconv_RFmap(glms[cc], data, valUT[cc], drift_mod=driftmods[cc], bootstrap=1)\n",
    "    \n",
    "    try:\n",
    "        con, RF_areas[cc], ctr = RU.get_contour(Gconv_space_all[cc,:,:].squeeze(), \n",
    "                                                                      thresh = 0.8*np.max(Gconv_space_col_shuff_all[cc,:,:,:]))\n",
    "        contours_all.append(con)\n",
    "    except:\n",
    "        contours_all.append([0])\n",
    "    \n",
    "    try:\n",
    "        con_col1, RF_areas_col[cc,0], ctr = RU.get_contour(Gconv_space_col_all[cc,0,:,:].squeeze(),\n",
    "                                                           thresh = 0.8*np.max(Gconv_space_col_shuff_all[cc,0,:,:]))\n",
    "    except: \n",
    "        con_col1=[0];\n",
    "        \n",
    "    try: \n",
    "        con_col2, RF_areas_col[cc,1], ctr = RU.get_contour(Gconv_space_col_all[cc,1,:,:].squeeze(), \n",
    "                                                                       thresh = 0.8*np.max(Gconv_space_col_shuff_all[cc,1,:,:]))\n",
    "    except: \n",
    "        con_col2=[0];\n",
    "    \n",
    "    try:\n",
    "        con_col3, RF_areas_col[cc,2], ctr = RU.get_contour(Gconv_space_col_all[cc,2,:,:].squeeze(), \n",
    "                                                                       thresh = 0.8*np.max(Gconv_space_col_shuff_all[cc,2,:,:]))\n",
    "    except: \n",
    "        con_col3=[0];\n",
    "        \n",
    "    contours_col_all.append([con_col1,con_col2,con_col3])  \n",
    "    print( \"Cell %3d of %d\"%(cc, NCv) )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67082ba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_ks = np.zeros([NCv, 3, NXglm, NXglm, num_lags]) \n",
    "for cc in range(NCv):\n",
    "    glm_ks[cc,:,:,:,:] = glms[cc].get_weights().squeeze()\n",
    "glm_ks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7af3c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sio.savemat(dirname2+'GLMT_measUT_ET.mat', {\n",
    "    'glm_ks':glm_ks, 'valUT':valUT, 'LLsGLM':LLsGLM[:, None],\n",
    "    'RFcenters': RFcenters, 'Colws': Gconv_cws, 'RFareas': RF_areas, 'RFareas_col':RF_areas_col,\n",
    "    'RFmaps': Gconv_space_all, 'RFmaps_col': Gconv_space_col_all, \n",
    "    'RFbs': Gconv_space_shuff_all, 'RFbs_col': Gconv_space_col_shuff_all, \n",
    "    'Contours':contours_all, 'Contours_col':contours_col_all})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a6fc3ef",
   "metadata": {},
   "source": [
    "## Now do NForm for completion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0dd6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NFloc = np.array([918, 515], dtype=np.int64)\n",
    "\n",
    "data.assemble_stimulus(top_corner=NFloc, fixdot=0, L=UNX, time_embed=0, shifts=-fixshifts)\n",
    "Reff = torch.mul(data.robs[:, valNF], data.dfs[:, valNF])\n",
    "nspks = torch.sum(Reff, axis=0)\n",
    "\n",
    "data.draw_stim_locations( top_corner=NFloc, L=UNX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1915945",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Calculate STAS\n",
    "lag = 4\n",
    "stasC = ((data.stim[:-lag, ...].T @ Reff[lag:,:]).squeeze() / nspks).reshape([3, UNX,UNX,-1]).numpy()\n",
    "stasC.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb55206",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctext = ['Lum', 'L-M', 'S']\n",
    "ss(6, 6, rh=4)\n",
    "for cc in range(NCNF):\n",
    "    for clr in range(3):\n",
    "        plt.subplot(6, 6, 3*cc+clr+1)\n",
    "        imagesc(stasC[clr, :, :, cc])\n",
    "        if clr == 0:\n",
    "            plt.ylabel( \"Cell %d\"%cc)\n",
    "        plt.title(ctext[clr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a82d685",
   "metadata": {},
   "outputs": [],
   "source": [
    "NCv=NCNF\n",
    "\n",
    "NXglm = 60\n",
    "num_lags = 8\n",
    "\n",
    "LLs0, LLsGLM, XTopt, GLopt = np.zeros([NCv,2]), np.zeros(NCv), np.zeros(NCv), np.zeros(NCv)\n",
    "glms = [None]*NCv\n",
    "driftmods = [None]*NCv\n",
    "Dreg = 0.1\n",
    "\n",
    "lbfgs_pars = utils.create_optimizer_params(\n",
    "    optimizer_type='lbfgs',\n",
    "    tolerance_change=1e-10,\n",
    "    tolerance_grad=1e-10,\n",
    "    batch_size=16000,\n",
    "    max_epochs=3,\n",
    "    max_iter = 200)\n",
    "#lbfgs_pars['num_workers'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3393f2ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "valNF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ade9ad9f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "XTreg = 20 # [20]\n",
    "L1reg = 0.1 # [0.5]\n",
    "GLreg = 4.0 # [4.0]\n",
    "\n",
    "glm_layer = NDNLayer.layer_dict( \n",
    "    input_dims=data.stim_dims, num_filters=1, bias=False,\n",
    "    NLtype='lin', initialize_center = False)\n",
    "glm_layer['reg_vals'] = {'d2xt': XTreg, 'l1': L1reg, 'glocalx': GLreg,'edge_t':100} \n",
    "\n",
    "stim_net =  FFnetwork.ffnet_dict( xstim_n = 'stim', layer_list = [glm_layer] )\n",
    "\n",
    "drift_pars1 = NDNLayer.layer_dict( \n",
    "    input_dims=[1,1,1,NA], num_filters=1, bias=False, norm_type=0, NLtype='lin')\n",
    "drift_pars1['reg_vals'] = {'d2t': Dreg, 'bcs':{'d2t':0} } \n",
    "\n",
    "# for stand-alone drift model\n",
    "drift_pars1N = deepcopy(drift_pars1)\n",
    "drift_pars1N['NLtype'] = 'softplus'\n",
    "\n",
    "drift_net =  FFnetwork.ffnet_dict( xstim_n = 'Xdrift', layer_list = [drift_pars1] )\n",
    "\n",
    "comb_layer = NDNLayer.layer_dict(\n",
    "    num_filters = 1, NLtype='softplus', bias=False)\n",
    "comb_layer['weights_initializer'] = 'ones'\n",
    "\n",
    "net_comb = FFnetwork.ffnet_dict( \n",
    "    xstim_n = None, ffnet_n=[0,1],\n",
    "    layer_list = [comb_layer], ffnet_type='add')\n",
    "\n",
    "#stim_net['layer_list'][0]['reg_vals']['d2xt'] = 10\n",
    "net_comb['layer_list'][0]['bias'] = True\n",
    "\n",
    "glms = [None]*NCv\n",
    "driftmods = [None]*NCv\n",
    "\n",
    "XTopt = np.zeros(NCv) \n",
    "GLopt = np.zeros(NCv) \n",
    "LLsNULL = np.zeros(NCv)\n",
    "LLsR = np.zeros([NCv,4])\n",
    "\n",
    "rvals = [0.001, 0.01, 0.1, 1, 10, 100, 1000, 10000]\n",
    "rvalsG = [0.001, 0.01, 0.01, 1, 10, 100, 1000] # glocal\n",
    "\n",
    "data.assemble_stimulus(top_corner=NFloc, fixdot=0, L=NXglm, time_embed=0, num_lags=num_lags, shifts=-fixshifts)\n",
    "\n",
    "for cc in range(NCv):\n",
    "    data.cells_out = [valNF[cc]]\n",
    "\n",
    "    drift_iter = NDN.NDN( \n",
    "        layer_list = [drift_pars1N], loss_type='poisson')\n",
    "    drift_iter.block_sample=True\n",
    "    drift_iter.networks[0].xstim_n = 'Xdrift'\n",
    "    drift_iter.fit( data, force_dict_training=True, train_inds=None, **lbfgs_pars, verbose=0, version=1)\n",
    "    LLsNULL[cc] = drift_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "    driftmods[cc] = deepcopy(drift_iter)\n",
    "\n",
    "    LLs = np.zeros(len(rvals))+LLsR[cc,0]\n",
    "    LLs = np.zeros(len(rvals))+LLsNULL[cc]\n",
    "    for rr in range(len(rvals)):\n",
    "        stim_net['layer_list'][0]['reg_vals']['d2xt'] = rvals[rr]\n",
    "        glm_iter = NDN.NDN(ffnet_list = [stim_net, drift_net, net_comb], loss_type='poisson')\n",
    "        glm_iter.block_sample=True\n",
    "        glm_iter.networks[1].layers[0].weight.data[:,0] = deepcopy(\n",
    "            driftmods[cc].networks[0].layers[0].weight.data[:,0])\n",
    "        glm_iter.networks[2].layers[0].set_parameters(val=False,name='weight')\n",
    "        glm_iter.networks[1].layers[0].set_parameters(val=False)\n",
    "\n",
    "        glm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, version=9, verbose=0)\n",
    "        LLi = glm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (rr == 0) or (LLi < np.min(LLs)):\n",
    "            best_model = deepcopy(glm_iter)\n",
    "        LLs[rr] = LLi\n",
    "    bm = np.argmin(LLs)\n",
    "    LLsR[cc,0] = LLs[bm]\n",
    "    XTopt[cc] = rvals[bm]\n",
    "    glms[cc] = deepcopy(best_model)\n",
    "    print( \"Cell %3d:  d2xt-R%d   LLs =\"%(cc, bm), LLsNULL[cc]-LLsR[cc,0] )\n",
    "\n",
    "    LLs = np.zeros(len(rvalsG))+LLsR[cc,0]\n",
    "    for rr in range(len(rvalsG)):\n",
    "        glm_iter = deepcopy(best_model)\n",
    "        glm_iter.networks[0].layers[0].reg.vals['glocalx'] = rvalsG[rr]\n",
    "        glm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = glm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_model = deepcopy(glm_iter)\n",
    "        LLs[rr] = LLi\n",
    "    bm = np.argmin(LLs)\n",
    "    LLsR[cc,1] = LLs[bm]\n",
    "    GLopt[cc] = rvals[bm]\n",
    "    glms[cc] = deepcopy(best_model)\n",
    "    print( \"Cell %3d:  Gloc-R%d   LL =%8.5f ->%8.5f\"%(cc, bm, LLsNULL[cc]-LLsR[cc,0], LLsNULL[cc]-LLsR[cc,1]) )\n",
    "    \n",
    "    LLs = np.zeros(len(rvals))+LLsR[cc,0]\n",
    "    for rr in range(len(rvals)):\n",
    "        glm_iter = deepcopy(best_model)\n",
    "        glm_iter.networks[0].layers[0].reg.vals['l1'] = rvals[rr]\n",
    "        glm_iter.fit( data, force_dict_training=True, **lbfgs_pars, seed=5, verbose=0)\n",
    "        LLi = glm_iter.eval_models(data[data.val_blks], null_adjusted=False)[0]\n",
    "        if (LLi < np.min(LLs)):\n",
    "            best_model = deepcopy(glm_iter)\n",
    "        LLs[rr] = LLi\n",
    "    bm = np.argmin(LLs)\n",
    "    LLsR[cc,2] = LLs[bm]\n",
    "    GLopt[cc] = rvals[bm]\n",
    "    glms[cc] = deepcopy(best_model)\n",
    "    print( \"Cell %3d:  L1  -R%d   LL =%8.5f ->%8.5f\"%(cc, bm, LLsNULL[cc]-LLsR[cc,0], LLsNULL[cc]-LLsR[cc,2]) )\n",
    "\n",
    "    utils.ss(3,7, row_height=4)\n",
    "    for ll in range(7):\n",
    "        plt.subplot(3,7,ll+1)\n",
    "        utils.imagesc(w[0,:,:,1+ll,0], aspect=1, max=np.max(w))\n",
    "\n",
    "        plt.subplot(3,7,ll+8)\n",
    "        utils.imagesc(w[1,:,:,1+ll,0], aspect=1, max=np.max(w))\n",
    "\n",
    "        plt.subplot(3,7,ll+15)\n",
    "        utils.imagesc(w[2,:,:,1+ll,0], aspect=1, max=np.max(w))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d825dbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble population drift terms\n",
    "w0 = driftmods[0].get_weights()\n",
    "NA = w0.shape[0]\n",
    "drift_terms = np.zeros([NA, NCv])\n",
    "for cc in range(NCv):\n",
    "    drift_terms[:, cc] = deepcopy(driftmods[cc].get_weights())[:,0]\n",
    "\n",
    "# Assemble drift population model\n",
    "drift_pars_pop = NDNLayer.layer_dict( \n",
    "    input_dims=[1,1,1,NA], num_filters=NCv, bias=False, norm_type=0, NLtype='softplus')\n",
    "drift_pars_pop['reg_vals'] = {'d2t': Dreg, 'bcs':{'d2t':0} } \n",
    "drift_pop = NDN.NDN( layer_list=[drift_pars_pop], loss_type='poisson')\n",
    "drift_pop.networks[0].xstim_n = 'Xdrift'\n",
    "drift_pop.networks[0].layers[0].weight.data = torch.tensor(drift_terms, dtype=torch.float32)\n",
    "data.set_cells(valNF)\n",
    "LLsCHECK = drift_pop.eval_models(data[data.val_blks], null_adjusted=False) \n",
    "print(np.mean(LLsNULL), np.mean(LLsCHECK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baa958ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get array centers\n",
    "RFcenters = np.zeros([NCv,2], dtype=np.int64) - 1\n",
    "for cc in range(NCv):\n",
    "    k = glms[cc].get_weights()\n",
    "    pfilt = np.sum(np.std(k, axis=3),axis=0).squeeze() # for colorRFs\n",
    "    x,y = utils.max_multiD( pfilt )\n",
    "    RFcenters[cc, :] = [x,y]   \n",
    "#    pfilt = np.std(k, axis=2).squeeze()\n",
    "#    x,y, snrs[cc] = CU.RFstd_evaluate( pfilt )\n",
    "#    RFcenters[cc, :] = [x,y]\n",
    "\n",
    "plt.plot(RFcenters[:, 0], RFcenters[:, 1],'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10fade2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate full GLM model like full drift model\n",
    "glm_layer = NDNLayer.layer_dict( \n",
    "    input_dims=[3, NXglm, NXglm, num_lags], num_filters=NCv, bias=False, num_lags=num_lags, NLtype='lin', initialize_center=True)\n",
    "glm_layer['reg_vals'] = {'d2xt': XTreg, 'l1': L1reg, 'glocalx': GLreg,'edge_t':100} \n",
    "\n",
    "stim_net =  FFnetwork.ffnet_dict( xstim_n='stim', layer_list=[glm_layer] )\n",
    "\n",
    "drift_pars = NDNLayer.layer_dict( \n",
    "    input_dims=[1,1,1,NA], num_filters=NCv, bias=False, norm_type=0, NLtype='lin')\n",
    "drift_pars['reg_vals'] = {'d2t': Dreg, 'bcs':{'d2t':0} } \n",
    "\n",
    "drift_net =  FFnetwork.ffnet_dict( xstim_n = 'Xdrift', layer_list=[drift_pars] )\n",
    "\n",
    "comb_layer = ChannelLayer.layer_dict( num_filters=NCv, NLtype='softplus', bias=True )\n",
    "comb_layer['weights_initializer'] = 'ones'\n",
    "\n",
    "net_comb = FFnetwork.ffnet_dict( \n",
    "    xstim_n = None, ffnet_n=[0,1],\n",
    "    layer_list = [comb_layer], ffnet_type='add')\n",
    "\n",
    "glm_all = NDN.NDN(  \n",
    "            ffnet_list = [stim_net, drift_net, net_comb], loss_type='poisson')\n",
    "glm_all.block_sample=True\n",
    "\n",
    "for cc in range(NCv):\n",
    "    glm_all.networks[0].layers[0].weight.data[:, cc] = deepcopy(glms[cc].networks[0].layers[0].weight.data[:,0])\n",
    "    glm_all.networks[1].layers[0].weight.data[:, cc] = deepcopy(glms[cc].networks[1].layers[0].weight.data[:,0])\n",
    "    glm_all.networks[-1].layers[0].bias.data[cc] = deepcopy(glms[cc].networks[-1].layers[0].bias.data)\n",
    "\n",
    "data.set_cells(valNF)\n",
    "LLsCHECK = glm_all.eval_models(data[data.val_blks], null_adjusted=False) \n",
    "print(np.mean(LLsNULL-LLsR[:,1]), np.mean(LLsNULL-LLsCHECK))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffbe4544",
   "metadata": {},
   "outputs": [],
   "source": [
    "LLsGLM = LLsNULL-LLsR[:,1]\n",
    "sio.savemat(dirname2+'LLsGLMTwET_NF.mat', {\n",
    "    'LLsNULL':LLsNULL[:,None], 'LLsGLM':LLsGLM[:, None], 'Dreg': Dreg,\n",
    "    'RFcenters': RFcenters, 'top_corner': NFloc})\n",
    "drift_pop.save_model(alt_dirname=dirname_mod, filename='ETdriftmods_popNF.pkl')\n",
    "glm_all.save_model(alt_dirname=dirname_mod, filename='ETglms_popNF.pkl')\n",
    "\n",
    "for cc in range(NCv):\n",
    "    glms[cc].save_model(\n",
    "        alt_dirname=dirname_mod, filename='NFglmT' + utils.filename_num2str(cc, num_digits=3) + 'wET_iter0.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f260611",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pure color weights\n",
    "Gconv_cws = np.zeros([NCv,3])\n",
    "Gconv_space_all = np.zeros([NCv,NXglm,NXglm])\n",
    "Gconv_space_col_all = np.zeros([NCv,3,NXglm,NXglm])\n",
    "Gconv_space_shuff_all = np.zeros([NCv,NXglm,NXglm])\n",
    "Gconv_space_col_shuff_all = np.zeros([NCv,3,NXglm,NXglm])\n",
    "\n",
    "RF_areas = np.zeros([NCv])\n",
    "RF_areas_col = np.zeros([NCv,3])\n",
    "contours_all = []\n",
    "contours_col_all = [] # dtype=object\n",
    "\n",
    "for cc in range(NCv):\n",
    "    data.cells_out = [valNF[cc]]\n",
    "    Gconv_cws[cc,0], Gconv_cws[cc,1], Gconv_cws[cc,2] = RU.get_Gconv_colws(glms[cc], data, valNF[cc])\n",
    "    [Gconv_space_all[cc,:,:], Gconv_space_col_all[cc,:,:,:]] = RU.get_Gconv_RFmap(glms[cc], data, valUT[cc], drift_mod=driftmods[cc])\n",
    "    [Gconv_space_shuff_all[cc,:,:], Gconv_space_col_shuff_all[cc,:,:,:]] = RU.get_Gconv_RFmap(glms[cc], data, valUT[cc], drift_mod=driftmods[cc], bootstrap=1)\n",
    "    \n",
    "    try:\n",
    "        con, RF_areas[cc], ctr = RU.get_contour(Gconv_space_all[cc,:,:].squeeze(), \n",
    "                                                                      thresh = 0.8*np.max(Gconv_space_col_shuff_all[cc,:,:,:]))\n",
    "        contours_all.append(con)\n",
    "    except:\n",
    "        contours_all.append([0])\n",
    "    \n",
    "    try:\n",
    "        con_col1, RF_areas_col[cc,0], ctr = RU.get_contour(Gconv_space_col_all[cc,0,:,:].squeeze(),\n",
    "                                                           thresh = 0.8*np.max(Gconv_space_col_shuff_all[cc,0,:,:]))\n",
    "    except: \n",
    "        con_col1=[0];\n",
    "        \n",
    "    try: \n",
    "        con_col2, RF_areas_col[cc,1], ctr = RU.get_contour(Gconv_space_col_all[cc,1,:,:].squeeze(), \n",
    "                                                                       thresh = 0.8*np.max(Gconv_space_col_shuff_all[cc,1,:,:]))\n",
    "    except: \n",
    "        con_col2=[0];\n",
    "    \n",
    "    try:\n",
    "        con_col3, RF_areas_col[cc,2], ctr = RU.get_contour(Gconv_space_col_all[cc,2,:,:].squeeze(), \n",
    "                                                                       thresh = 0.8*np.max(Gconv_space_col_shuff_all[cc,2,:,:]))\n",
    "    except: \n",
    "        con_col3=[0];\n",
    "        \n",
    "    contours_col_all.append([con_col1,con_col2,con_col3])  \n",
    "    print( \"Cell %3d of %d\"%(cc, NCv) )\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05cfa1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "glm_ks = np.zeros([NCv, 3, NXglm, NXglm, num_lags]) \n",
    "for cc in range(NCv):\n",
    "    glm_ks[cc,:,:,:,:] = glms[cc].get_weights().squeeze()\n",
    "glm_ks.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e7d20efc",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sio' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43msio\u001b[49m\u001b[38;5;241m.\u001b[39msavemat(dirname2\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGLMT_measNF_ET.mat\u001b[39m\u001b[38;5;124m'\u001b[39m, {\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mglm_ks\u001b[39m\u001b[38;5;124m'\u001b[39m:glm_ks, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvalNF\u001b[39m\u001b[38;5;124m'\u001b[39m:valNF, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLLsGLM\u001b[39m\u001b[38;5;124m'\u001b[39m:LLsGLM[:, \u001b[38;5;28;01mNone\u001b[39;00m],\n\u001b[1;32m      3\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRFcenters\u001b[39m\u001b[38;5;124m'\u001b[39m: RFcenters, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mColws\u001b[39m\u001b[38;5;124m'\u001b[39m: Gconv_cws, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRFareas\u001b[39m\u001b[38;5;124m'\u001b[39m: RF_areas, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRFareas_col\u001b[39m\u001b[38;5;124m'\u001b[39m:RF_areas_col,\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRFmaps\u001b[39m\u001b[38;5;124m'\u001b[39m: Gconv_space_all, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRFmaps_col\u001b[39m\u001b[38;5;124m'\u001b[39m: Gconv_space_col_all, \n\u001b[1;32m      5\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRFbs\u001b[39m\u001b[38;5;124m'\u001b[39m: Gconv_space_shuff_all, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRFbs_col\u001b[39m\u001b[38;5;124m'\u001b[39m: Gconv_space_col_shuff_all, \n\u001b[1;32m      6\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContours\u001b[39m\u001b[38;5;124m'\u001b[39m:contours_all, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mContours_col\u001b[39m\u001b[38;5;124m'\u001b[39m:contours_col_all})\n",
      "\u001b[0;31mNameError\u001b[0m: name 'sio' is not defined"
     ]
    }
   ],
   "source": [
    "sio.savemat(dirname2+'GLMT_measNF_ET.mat', {\n",
    "    'glm_ks':glm_ks, 'valNF':valNF, 'LLsGLM':LLsGLM[:, None],\n",
    "    'RFcenters': RFcenters, 'Colws': Gconv_cws, 'RFareas': RF_areas, 'RFareas_col':RF_areas_col,\n",
    "    'RFmaps': Gconv_space_all, 'RFmaps_col': Gconv_space_col_all, \n",
    "    'RFbs': Gconv_space_shuff_all, 'RFbs_col': Gconv_space_col_shuff_all, \n",
    "    'Contours':contours_all, 'Contours_col':contours_col_all})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723d5349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch38",
   "language": "python",
   "name": "torch38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
